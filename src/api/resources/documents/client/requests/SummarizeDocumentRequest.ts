/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         llm_name: "llm_name"
 *     }
 */
export interface SummarizeDocumentRequest {
    /**
     * The API will make a best effort to complete the request in the specified seconds or time out.
     */
    "Request-Timeout"?: number;
    /**
     * The API will make a best effort to complete the request in the specified milliseconds or time out.
     */
    "Request-Timeout-Millis"?: number;
    /** The name of the LLM. */
    llm_name: string;
    /** The prompt template to use when generating the summary. Vectara manages both system and user roles and prompts for the generative LLM out of the box by default. However, users can override the `prompt_template` via this variable. The `prompt_template` is in the form of an Apache Velocity template. For more details on how to configure the `prompt_template`, see the [long-form documentation](https://docs.vectara.com/docs/prompts/vectara-prompt-engine). */
    prompt_template?: string;
    /** Optional parameters for the specified model used when generating the summary. */
    model_parameters?: Record<string, unknown>;
    /** Indicates whether the response should be streamed or not. */
    stream_response?: boolean;
}
