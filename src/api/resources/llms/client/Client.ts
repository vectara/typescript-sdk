/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as environments from "../../../../environments.js";
import * as core from "../../../../core/index.js";
import * as Vectara from "../../../index.js";
import { mergeHeaders, mergeOnlyDefinedHeaders } from "../../../../core/headers.js";
import * as errors from "../../../../errors/index.js";

export declare namespace Llms {
    export interface Options {
        environment?: core.Supplier<environments.VectaraEnvironment | environments.VectaraEnvironmentUrls>;
        /** Specify a custom URL to connect the client to. */
        baseUrl?: core.Supplier<string>;
        token?: core.Supplier<core.BearerToken | undefined>;
        /** Override the x-api-key header */
        apiKey?: core.Supplier<string | undefined>;
        /** Additional headers to include in requests. */
        headers?: Record<string, string | core.Supplier<string | undefined> | undefined>;
        fetcher?: core.FetchFunction;
    }

    export interface RequestOptions {
        /** The maximum time to wait for a response in seconds. */
        timeoutInSeconds?: number;
        /** The number of times to retry the request. Defaults to 2. */
        maxRetries?: number;
        /** A hook to abort the request. */
        abortSignal?: AbortSignal;
        /** Override the x-api-key header */
        apiKey?: string | undefined;
        /** Additional headers to include in the request. */
        headers?: Record<string, string | core.Supplier<string | undefined> | undefined>;
    }
}

export class Llms {
    protected readonly _options: Llms.Options;

    constructor(_options: Llms.Options = {}) {
        this._options = _options;
    }

    /**
     * List LLMs that can be used with query and chat endpoints. The LLM is not directly specified in a query, but instead a `generation_preset_name` is used. The `generation_preset_name` property in generation parameters can be found as the `name` property on the Generations Presets retrieved from `/v2/generation_presets`.
     *
     * @param {Vectara.LlmsListRequest} request
     * @param {Llms.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Vectara.ForbiddenError}
     *
     * @example
     *     await client.llms.list()
     */
    public async list(
        request: Vectara.LlmsListRequest = {},
        requestOptions?: Llms.RequestOptions,
    ): Promise<core.Page<Vectara.Llm>> {
        const list = core.HttpResponsePromise.interceptFunction(
            async (request: Vectara.LlmsListRequest): Promise<core.WithRawResponse<Vectara.ListLlMsResponse>> => {
                const {
                    filter,
                    limit,
                    page_key: pageKey,
                    "Request-Timeout": requestTimeout,
                    "Request-Timeout-Millis": requestTimeoutMillis,
                } = request;
                const _queryParams: Record<string, string | string[] | object | object[] | null> = {};
                if (filter != null) {
                    _queryParams["filter"] = filter;
                }
                if (limit != null) {
                    _queryParams["limit"] = limit.toString();
                }
                if (pageKey != null) {
                    _queryParams["page_key"] = pageKey;
                }
                const _response = await (this._options.fetcher ?? core.fetcher)({
                    url: core.url.join(
                        (await core.Supplier.get(this._options.baseUrl)) ??
                            (
                                (await core.Supplier.get(this._options.environment)) ??
                                environments.VectaraEnvironment.Production
                            ).default,
                        "v2/llms",
                    ),
                    method: "GET",
                    headers: mergeHeaders(
                        this._options?.headers,
                        mergeOnlyDefinedHeaders({
                            Authorization: await this._getAuthorizationHeader(),
                            "Request-Timeout": requestTimeout != null ? requestTimeout.toString() : undefined,
                            "Request-Timeout-Millis":
                                requestTimeoutMillis != null ? requestTimeoutMillis.toString() : undefined,
                            "x-api-key": requestOptions?.apiKey,
                        }),
                        requestOptions?.headers,
                    ),
                    queryParameters: _queryParams,
                    timeoutMs:
                        requestOptions?.timeoutInSeconds != null ? requestOptions.timeoutInSeconds * 1000 : 60000,
                    maxRetries: requestOptions?.maxRetries,
                    abortSignal: requestOptions?.abortSignal,
                });
                if (_response.ok) {
                    return { data: _response.body as Vectara.ListLlMsResponse, rawResponse: _response.rawResponse };
                }
                if (_response.error.reason === "status-code") {
                    switch (_response.error.statusCode) {
                        case 403:
                            throw new Vectara.ForbiddenError(
                                _response.error.body as Vectara.Error_,
                                _response.rawResponse,
                            );
                        default:
                            throw new errors.VectaraError({
                                statusCode: _response.error.statusCode,
                                body: _response.error.body,
                                rawResponse: _response.rawResponse,
                            });
                    }
                }
                switch (_response.error.reason) {
                    case "non-json":
                        throw new errors.VectaraError({
                            statusCode: _response.error.statusCode,
                            body: _response.error.rawBody,
                            rawResponse: _response.rawResponse,
                        });
                    case "timeout":
                        throw new errors.VectaraTimeoutError("Timeout exceeded when calling GET /v2/llms.");
                    case "unknown":
                        throw new errors.VectaraError({
                            message: _response.error.errorMessage,
                            rawResponse: _response.rawResponse,
                        });
                }
            },
        );
        const dataWithRawResponse = await list(request).withRawResponse();
        return new core.Pageable<Vectara.ListLlMsResponse, Vectara.Llm>({
            response: dataWithRawResponse.data,
            rawResponse: dataWithRawResponse.rawResponse,
            hasNextPage: (response) =>
                response?.metadata?.page_key != null &&
                !(typeof response?.metadata?.page_key === "string" && response?.metadata?.page_key === ""),
            getItems: (response) => response?.llms ?? [],
            loadPage: (response) => {
                return list(core.setObjectProperty(request, "page_key", response?.metadata?.page_key));
            },
        });
    }

    /**
     * Create a new LLM for use with query and chat endpoints
     *
     * @param {Vectara.LlmsCreateRequest} request
     * @param {Llms.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Vectara.BadRequestError}
     * @throws {@link Vectara.ForbiddenError}
     *
     * @example
     *     await client.llms.create({
     *         body: {
     *             type: "openai-compatible",
     *             name: "name",
     *             model: "model",
     *             uri: "uri"
     *         }
     *     })
     */
    public create(
        request: Vectara.LlmsCreateRequest,
        requestOptions?: Llms.RequestOptions,
    ): core.HttpResponsePromise<Vectara.Llm> {
        return core.HttpResponsePromise.fromPromise(this.__create(request, requestOptions));
    }

    private async __create(
        request: Vectara.LlmsCreateRequest,
        requestOptions?: Llms.RequestOptions,
    ): Promise<core.WithRawResponse<Vectara.Llm>> {
        const {
            "Request-Timeout": requestTimeout,
            "Request-Timeout-Millis": requestTimeoutMillis,
            body: _body,
        } = request;
        const _response = await (this._options.fetcher ?? core.fetcher)({
            url: core.url.join(
                (await core.Supplier.get(this._options.baseUrl)) ??
                    ((await core.Supplier.get(this._options.environment)) ?? environments.VectaraEnvironment.Production)
                        .default,
                "v2/llms",
            ),
            method: "POST",
            headers: mergeHeaders(
                this._options?.headers,
                mergeOnlyDefinedHeaders({
                    Authorization: await this._getAuthorizationHeader(),
                    "Request-Timeout": requestTimeout != null ? requestTimeout.toString() : undefined,
                    "Request-Timeout-Millis":
                        requestTimeoutMillis != null ? requestTimeoutMillis.toString() : undefined,
                    "x-api-key": requestOptions?.apiKey,
                }),
                requestOptions?.headers,
            ),
            contentType: "application/json",
            requestType: "json",
            body: _body,
            timeoutMs: requestOptions?.timeoutInSeconds != null ? requestOptions.timeoutInSeconds * 1000 : 60000,
            maxRetries: requestOptions?.maxRetries,
            abortSignal: requestOptions?.abortSignal,
        });
        if (_response.ok) {
            return { data: _response.body as Vectara.Llm, rawResponse: _response.rawResponse };
        }

        if (_response.error.reason === "status-code") {
            switch (_response.error.statusCode) {
                case 400:
                    throw new Vectara.BadRequestError(
                        _response.error.body as Vectara.BadRequestErrorBody,
                        _response.rawResponse,
                    );
                case 403:
                    throw new Vectara.ForbiddenError(_response.error.body as Vectara.Error_, _response.rawResponse);
                default:
                    throw new errors.VectaraError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                        rawResponse: _response.rawResponse,
                    });
            }
        }

        switch (_response.error.reason) {
            case "non-json":
                throw new errors.VectaraError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                    rawResponse: _response.rawResponse,
                });
            case "timeout":
                throw new errors.VectaraTimeoutError("Timeout exceeded when calling POST /v2/llms.");
            case "unknown":
                throw new errors.VectaraError({
                    message: _response.error.errorMessage,
                    rawResponse: _response.rawResponse,
                });
        }
    }

    /**
     * Get details about a specific LLM.
     *
     * @param {string} llmId - The name of the LLM to retrieve.
     * @param {Vectara.LlmsGetRequest} request
     * @param {Llms.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Vectara.ForbiddenError}
     * @throws {@link Vectara.NotFoundError}
     *
     * @example
     *     await client.llms.get("llm_id")
     */
    public get(
        llmId: string,
        request: Vectara.LlmsGetRequest = {},
        requestOptions?: Llms.RequestOptions,
    ): core.HttpResponsePromise<Vectara.Llm> {
        return core.HttpResponsePromise.fromPromise(this.__get(llmId, request, requestOptions));
    }

    private async __get(
        llmId: string,
        request: Vectara.LlmsGetRequest = {},
        requestOptions?: Llms.RequestOptions,
    ): Promise<core.WithRawResponse<Vectara.Llm>> {
        const { "Request-Timeout": requestTimeout, "Request-Timeout-Millis": requestTimeoutMillis } = request;
        const _response = await (this._options.fetcher ?? core.fetcher)({
            url: core.url.join(
                (await core.Supplier.get(this._options.baseUrl)) ??
                    ((await core.Supplier.get(this._options.environment)) ?? environments.VectaraEnvironment.Production)
                        .default,
                `v2/llms/${encodeURIComponent(llmId)}`,
            ),
            method: "GET",
            headers: mergeHeaders(
                this._options?.headers,
                mergeOnlyDefinedHeaders({
                    Authorization: await this._getAuthorizationHeader(),
                    "Request-Timeout": requestTimeout != null ? requestTimeout.toString() : undefined,
                    "Request-Timeout-Millis":
                        requestTimeoutMillis != null ? requestTimeoutMillis.toString() : undefined,
                    "x-api-key": requestOptions?.apiKey,
                }),
                requestOptions?.headers,
            ),
            timeoutMs: requestOptions?.timeoutInSeconds != null ? requestOptions.timeoutInSeconds * 1000 : 60000,
            maxRetries: requestOptions?.maxRetries,
            abortSignal: requestOptions?.abortSignal,
        });
        if (_response.ok) {
            return { data: _response.body as Vectara.Llm, rawResponse: _response.rawResponse };
        }

        if (_response.error.reason === "status-code") {
            switch (_response.error.statusCode) {
                case 403:
                    throw new Vectara.ForbiddenError(_response.error.body as Vectara.Error_, _response.rawResponse);
                case 404:
                    throw new Vectara.NotFoundError(
                        _response.error.body as Vectara.NotFoundErrorBody,
                        _response.rawResponse,
                    );
                default:
                    throw new errors.VectaraError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                        rawResponse: _response.rawResponse,
                    });
            }
        }

        switch (_response.error.reason) {
            case "non-json":
                throw new errors.VectaraError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                    rawResponse: _response.rawResponse,
                });
            case "timeout":
                throw new errors.VectaraTimeoutError("Timeout exceeded when calling GET /v2/llms/{llm_id}.");
            case "unknown":
                throw new errors.VectaraError({
                    message: _response.error.errorMessage,
                    rawResponse: _response.rawResponse,
                });
        }
    }

    /**
     * Delete a custom LLM connection. Built-in LLMs cannot be deleted.
     *
     * @param {string} llmId - The name of the LLM to delete.
     * @param {Vectara.LlmsDeleteRequest} request
     * @param {Llms.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Vectara.ForbiddenError}
     * @throws {@link Vectara.NotFoundError}
     *
     * @example
     *     await client.llms.delete("llm_id")
     */
    public delete(
        llmId: string,
        request: Vectara.LlmsDeleteRequest = {},
        requestOptions?: Llms.RequestOptions,
    ): core.HttpResponsePromise<void> {
        return core.HttpResponsePromise.fromPromise(this.__delete(llmId, request, requestOptions));
    }

    private async __delete(
        llmId: string,
        request: Vectara.LlmsDeleteRequest = {},
        requestOptions?: Llms.RequestOptions,
    ): Promise<core.WithRawResponse<void>> {
        const { "Request-Timeout": requestTimeout, "Request-Timeout-Millis": requestTimeoutMillis } = request;
        const _response = await (this._options.fetcher ?? core.fetcher)({
            url: core.url.join(
                (await core.Supplier.get(this._options.baseUrl)) ??
                    ((await core.Supplier.get(this._options.environment)) ?? environments.VectaraEnvironment.Production)
                        .default,
                `v2/llms/${encodeURIComponent(llmId)}`,
            ),
            method: "DELETE",
            headers: mergeHeaders(
                this._options?.headers,
                mergeOnlyDefinedHeaders({
                    Authorization: await this._getAuthorizationHeader(),
                    "Request-Timeout": requestTimeout != null ? requestTimeout.toString() : undefined,
                    "Request-Timeout-Millis":
                        requestTimeoutMillis != null ? requestTimeoutMillis.toString() : undefined,
                    "x-api-key": requestOptions?.apiKey,
                }),
                requestOptions?.headers,
            ),
            timeoutMs: requestOptions?.timeoutInSeconds != null ? requestOptions.timeoutInSeconds * 1000 : 60000,
            maxRetries: requestOptions?.maxRetries,
            abortSignal: requestOptions?.abortSignal,
        });
        if (_response.ok) {
            return { data: undefined, rawResponse: _response.rawResponse };
        }

        if (_response.error.reason === "status-code") {
            switch (_response.error.statusCode) {
                case 403:
                    throw new Vectara.ForbiddenError(_response.error.body as Vectara.Error_, _response.rawResponse);
                case 404:
                    throw new Vectara.NotFoundError(
                        _response.error.body as Vectara.NotFoundErrorBody,
                        _response.rawResponse,
                    );
                default:
                    throw new errors.VectaraError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                        rawResponse: _response.rawResponse,
                    });
            }
        }

        switch (_response.error.reason) {
            case "non-json":
                throw new errors.VectaraError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                    rawResponse: _response.rawResponse,
                });
            case "timeout":
                throw new errors.VectaraTimeoutError("Timeout exceeded when calling DELETE /v2/llms/{llm_id}.");
            case "unknown":
                throw new errors.VectaraError({
                    message: _response.error.errorMessage,
                    rawResponse: _response.rawResponse,
                });
        }
    }

    protected async _getAuthorizationHeader(): Promise<string | undefined> {
        const bearer = await core.Supplier.get(this._options.token);
        if (bearer != null) {
            return `Bearer ${bearer}`;
        }

        return undefined;
    }
}
