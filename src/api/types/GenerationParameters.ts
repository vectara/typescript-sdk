/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vectara from "../index.js";

/**
 * The parameters to control generation.
 */
export interface GenerationParameters {
    /**
     * The preset values to use to feed the query results and other context to the model.
     * A `generation_preset` is an object with a bundle of properties that specifies: * The `prompt_template` that is rendered and then sent to the LLM. * The LLM used. * `model_parameter`s such as temperature.
     * All of these properties except the model can be overridden by setting them in this object. Even when a `prompt_template` is set, the `generation_preset_name` is used to set the model used. See `model_parameters.model` if you want to set the model explicitly.
     * If `generation_preset_name` is not set, the Vectara platform will use the default model and prompt.
     */
    generation_preset_name?: string;
    /** Use `generation_preset_name` instead of `prompt_name`. */
    prompt_name?: string;
    /** The maximum number of search results to be available to the prompt. */
    max_used_search_results?: number;
    /** Vectara manages both system and user roles and prompts for the generative LLM out of the box by default. However, users can override the `prompt_template` via this variable. The `prompt_template` is in the form of an Apache Velocity template. For more details on how to configure the `prompt_template`, see the [long-form documentation](https://docs.vectara.com/docs/prompts/vectara-prompt-engine). */
    prompt_template?: string;
    /** This property is deprecated in favor of clearer naming. Use `prompt_template`. This property will be ignored if `prompt_template` is set. */
    prompt_text?: string;
    /** Controls the length of the generated output. This is a rough estimate and not a hard limit: the end output can be longer or shorter than this value. This is generally implemented by including the `max_response_characters` in the prompt, and the LLM's instruction following capability dictates how closely the generated output is limited. */
    max_response_characters?: number;
    response_language?: Vectara.Language;
    /** The parameters for the model. WARNING: This is an experimental feature, and breakable at any point with virtually no notice. It is meant for experimentation to converge on optimal parameters that can then be set in the prompt definitions. */
    model_parameters?: GenerationParameters.ModelParameters;
    citations?: Vectara.CitationParameters;
    /** Enable returning the factual consistency score with query results. */
    enable_factual_consistency_score?: boolean;
}

export namespace GenerationParameters {
    /**
     * The parameters for the model. WARNING: This is an experimental feature, and breakable at any point with virtually no notice. It is meant for experimentation to converge on optimal parameters that can then be set in the prompt definitions.
     */
    export interface ModelParameters {
        /** The model (e.g., `gpt-4`) to use for summarization. If specified, it will override the model behind `generation_preset_name`. */
        llm_name?: string;
        /** The maximum number of tokens to be returned by the model. */
        max_tokens?: number;
        /** The sampling temperature to use. Higher values make the output more random, while lower values make it more focused and deterministic. */
        temperature?: number;
        /** Higher values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. */
        frequency_penalty?: number;
        /** Higher values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. */
        presence_penalty?: number;
    }
}
